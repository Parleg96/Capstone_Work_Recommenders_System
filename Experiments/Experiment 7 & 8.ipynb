{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8302743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5685bf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49205e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Priyanka G\\\\CAPSTONE_PROJECT_WORK(2022)\\\\panneer'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb5747e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\Priyanka G\\\\CAPSTONE_PROJECT_WORK(2022)\\\\panneer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03f15cf",
   "metadata": {},
   "source": [
    "##### train the model on train set\n",
    "    model = SVC()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # print prediction results\n",
    "    predictions = model.predict(X_test)\n",
    "    print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60cc069",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51895a16",
   "metadata": {},
   "source": [
    "## focus only on the category cake \n",
    "for each dish extract the cleaned web content using the urls for that dish. Save these files as individual notebooks in a folder \n",
    "cake-subcategroy-file or each dish undder this category - under each dish data to be stored as separate text files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff7d7fd",
   "metadata": {},
   "source": [
    "##### 1 > train the model on data1 ,validate the model on the other data. \n",
    "    o/p table : \n",
    "    1c data 2c precisition 3c recall 4c no of 0's and no of 1's 5c f1 score\n",
    "##### 2 >train the models on 4 randomly choosen data collated together\n",
    "      use CV to choose appropriate hyperparameters\n",
    "      Test the model\n",
    "      1>on the remaining 4 data individually\n",
    "      2> on a single test data created by appending the 4 test data\n",
    "##### 3>Test the model on a dataset on gobi and chicken manchurian\n",
    "       \n",
    "       \n",
    "#### The model sequence\n",
    "     1>DT\n",
    "     2>RF\n",
    "     3>Adaboost\n",
    "     4>SVM\n",
    "     5>GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fdee3f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "754ae233",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('site1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b83ee5da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>newline_after</th>\n",
       "      <th>newline_before</th>\n",
       "      <th>nwords</th>\n",
       "      <th>numbers</th>\n",
       "      <th>nnumbers</th>\n",
       "      <th>nnouns</th>\n",
       "      <th>nverbs</th>\n",
       "      <th>nunits</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>skip to primary navigation</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>skip to main content</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>skip to primary sidebar</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yummy tummy https //wwwyummytummyaarthicom/wp</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>content/uploads/2017/05/tummypng</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>recipe index</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>newsletter</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>sign up</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>for emails and updates</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>stay connected</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>501 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Sentence  newline_after  \\\n",
       "0                       skip to primary navigation              1   \n",
       "1                             skip to main content              1   \n",
       "2                          skip to primary sidebar              0   \n",
       "3    yummy tummy https //wwwyummytummyaarthicom/wp              1   \n",
       "4                 content/uploads/2017/05/tummypng              2   \n",
       "..                                             ...            ...   \n",
       "496                                   recipe index              2   \n",
       "497                                     newsletter              2   \n",
       "498                                        sign up              0   \n",
       "499                         for emails and updates              2   \n",
       "500                                 stay connected              2   \n",
       "\n",
       "     newline_before  nwords  numbers  nnumbers  nnouns  nverbs  nunits   label  \n",
       "0                 0       4        0         0       2       0        0      0  \n",
       "1                 1       4        0         0       2       0        0      0  \n",
       "2                 1       4        0         0       2       0        0      0  \n",
       "3                 0       4        0         0       4       0        0      0  \n",
       "4                 1       1        1         6       1       0        0      0  \n",
       "..              ...     ...      ...       ...     ...     ...      ...    ...  \n",
       "496               1       2        0         0       2       0        0      0  \n",
       "497               2       1        0         0       1       0        0      0  \n",
       "498               2       2        0         0       1       0        0      0  \n",
       "499               0       4        0         0       2       0        0      0  \n",
       "500               2       2        0         0       1       1        0      0  \n",
       "\n",
       "[501 rows x 10 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2207fa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data1.drop([\"Sentence\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a96e1a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data1.loc[data1.nwords > 2, :]\n",
    "data1 = data1.loc[data1.nnumbers <= 3, :]\n",
    "data1 = data1.loc[data1.nwords <= 10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "672a1a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble as ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c5c1a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data1.drop(\"label\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35d210cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data1.label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359419ce",
   "metadata": {},
   "source": [
    "#### 1 > train the model on data1 ,validate the model on the other data.¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37377c78",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bb7b0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import  DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef35e110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=3, min_samples_leaf=5)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=DecisionTreeClassifier( max_depth=3, min_samples_leaf=5)\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e63b109",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('site2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827a1e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20445784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>newline_after</th>\n",
       "      <th>newline_before</th>\n",
       "      <th>nwords</th>\n",
       "      <th>numbers</th>\n",
       "      <th>nnumbers</th>\n",
       "      <th>nnouns</th>\n",
       "      <th>nverbs</th>\n",
       "      <th>nunits</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>721.000000</td>\n",
       "      <td>721.000000</td>\n",
       "      <td>721.000000</td>\n",
       "      <td>721.000000</td>\n",
       "      <td>721.000000</td>\n",
       "      <td>721.000000</td>\n",
       "      <td>721.000000</td>\n",
       "      <td>721.000000</td>\n",
       "      <td>721.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.208044</td>\n",
       "      <td>1.205270</td>\n",
       "      <td>7.359223</td>\n",
       "      <td>0.380028</td>\n",
       "      <td>3.524272</td>\n",
       "      <td>2.837725</td>\n",
       "      <td>0.919556</td>\n",
       "      <td>0.113731</td>\n",
       "      <td>0.059639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.715691</td>\n",
       "      <td>0.716492</td>\n",
       "      <td>6.014099</td>\n",
       "      <td>0.485730</td>\n",
       "      <td>7.560664</td>\n",
       "      <td>1.900297</td>\n",
       "      <td>1.175759</td>\n",
       "      <td>0.317705</td>\n",
       "      <td>0.236982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       newline_after  newline_before      nwords     numbers    nnumbers  \\\n",
       "count     721.000000      721.000000  721.000000  721.000000  721.000000   \n",
       "mean        1.208044        1.205270    7.359223    0.380028    3.524272   \n",
       "std         0.715691        0.716492    6.014099    0.485730    7.560664   \n",
       "min         0.000000        0.000000    1.000000    0.000000    0.000000   \n",
       "25%         1.000000        1.000000    3.000000    0.000000    0.000000   \n",
       "50%         1.000000        1.000000    5.000000    0.000000    0.000000   \n",
       "75%         2.000000        2.000000   12.000000    1.000000    2.000000   \n",
       "max         4.000000        4.000000   30.000000    1.000000   27.000000   \n",
       "\n",
       "           nnouns      nverbs     nunits        label  \n",
       "count  721.000000  721.000000  721.000000  721.000000  \n",
       "mean     2.837725    0.919556    0.113731    0.059639  \n",
       "std      1.900297    1.175759    0.317705    0.236982  \n",
       "min      0.000000    0.000000    0.000000    0.000000  \n",
       "25%      1.000000    0.000000    0.000000    0.000000  \n",
       "50%      3.000000    1.000000    0.000000    0.000000  \n",
       "75%      4.000000    2.000000    0.000000    0.000000  \n",
       "max     12.000000    7.000000    1.000000    1.000000  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da6e3dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2['label'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc25488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58a46c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(data2.Sentence[pred == 1])                     ###predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96c6a203",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(data2.Sentence[data2.label == 1])             ## actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83029146",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data2.drop([\"Sentence\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26d1df53",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = data2.drop(\"label\",axis=1)\n",
    "y_test = data2.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e8c21db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 221, 1: 37})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e2258ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "708c96d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>215</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    0   1\n",
       "label         \n",
       "0      215   6\n",
       "1        7  30"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fcb5780c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[215,   6],\n",
       "       [  7,  30]], dtype=int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "99cf1af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "46efdb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prescision :  0.8333333333333334\n",
      "Recall :  0.8108108108108109\n"
     ]
    }
   ],
   "source": [
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a3fbe169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score :  0.8219178082191781\n"
     ]
    }
   ],
   "source": [
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa80c5f",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6bbb67a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_features=0.5, n_estimators=50, oob_score=True)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomForest = ensemble.RandomForestClassifier(n_estimators=50,max_features=0.5,oob_score=True)\n",
    "randomForest.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ba1b8447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomForest.base_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "818c5a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = randomForest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "01fb1c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9939024390243902"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomForest.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "64cee181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[211,  10],\n",
       "       [  6,  31]], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f2826693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prescision :  0.7560975609756098\n",
      "Recall :  0.8378378378378378\n"
     ]
    }
   ],
   "source": [
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred1))\n",
    "print(\"Recall : \" , recall_score(y_test, pred1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bf596d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score :  0.7948717948717948\n"
     ]
    }
   ],
   "source": [
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5169f4f2",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b964959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "52046332",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(n_estimators=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "68448da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(n_estimators=100, random_state=0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2a99c8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4cf3db1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[211,  10],\n",
       "       [  9,  28]], dtype=int64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "64852c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prescision :  0.7368421052631579\n",
      "Recall :  0.7567567567567568\n"
     ]
    }
   ],
   "source": [
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred2))\n",
    "print(\"Recall : \" , recall_score(y_test, pred2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ac302193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score :  0.7466666666666667\n"
     ]
    }
   ],
   "source": [
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d02fa63",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "43024851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf1 = svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ba187d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b7851611",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred3 = clf1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1b1aa6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[218,   3],\n",
       "       [ 15,  22]], dtype=int64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7d00efb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prescision :  0.88\n",
      "Recall :  0.5945945945945946\n"
     ]
    }
   ],
   "source": [
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred3))\n",
    "print(\"Recall : \" , recall_score(y_test, pred3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "34897a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score :  0.7096774193548386\n"
     ]
    }
   ],
   "source": [
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f736ee6",
   "metadata": {},
   "source": [
    "### GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8f15fba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e6c7e1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d26955c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred4 = clf2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "df70293f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[207,  14],\n",
       "       [  6,  31]], dtype=int64)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3552692c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prescision :  0.6888888888888889\n",
      "Recall :  0.8378378378378378\n"
     ]
    }
   ],
   "source": [
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred4))\n",
    "print(\"Recall : \" , recall_score(y_test, pred4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8d41cf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score :  0.7560975609756098\n"
     ]
    }
   ],
   "source": [
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "2ce14fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('d:\\\\spark_install\\\\lib\\\\site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8e24771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    " \n",
    "# Specify the Column Names while initializing the Table\n",
    "myTable = PrettyTable([\"Model\",\"Site_Name\", \"Precision\", \"Recall\", \"F1_score\",\"No of 0's\",\"No of 1's\",\"Predicted 0's\",\"Predicted1's\"])\n",
    "myTable.add_row([\"Decision Tree\",\"Site2\", \"0.834\",\"0.8109\", \"0.821\",\"221\",\"37\",\"215\",\"30\"])\n",
    "myTable.add_row([\"Random Forest\",\"Site2\", \"0.775\",\"0.837\", \"0.805\",\"221\",\"37\",\"211\",\"31\"])\n",
    "myTable.add_row([\"Adaboost\",\"Site2\", \"0.736\",\"0.756\", \"0.746\",\"221\",\"37\",\"211\",\"28\"])\n",
    "\n",
    "myTable.add_row([\"SVM\",\"Site2\", \"0.88\",\"0.594\", \"0.709\",\"221\",\"37\",\"218\",\"22\"])\n",
    "myTable.add_row([\"GBM\",\"Site2\", \"0.689\",\"0.837\", \"0.756\",\"221\",\"37\",\"207\",\"31\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8da9e939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Model</th>\n",
       "            <th>Site_Name</th>\n",
       "            <th>Precision</th>\n",
       "            <th>Recall</th>\n",
       "            <th>F1_score</th>\n",
       "            <th>No of 0&#x27;s</th>\n",
       "            <th>No of 1&#x27;s</th>\n",
       "            <th>Predicted 0&#x27;s</th>\n",
       "            <th>Predicted1&#x27;s</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>Decision Tree</td>\n",
       "            <td>Site2</td>\n",
       "            <td>0.834</td>\n",
       "            <td>0.8109</td>\n",
       "            <td>0.821</td>\n",
       "            <td>221</td>\n",
       "            <td>37</td>\n",
       "            <td>215</td>\n",
       "            <td>30</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Random Forest</td>\n",
       "            <td>Site2</td>\n",
       "            <td>0.775</td>\n",
       "            <td>0.837</td>\n",
       "            <td>0.805</td>\n",
       "            <td>221</td>\n",
       "            <td>37</td>\n",
       "            <td>211</td>\n",
       "            <td>31</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Adaboost</td>\n",
       "            <td>Site2</td>\n",
       "            <td>0.736</td>\n",
       "            <td>0.756</td>\n",
       "            <td>0.746</td>\n",
       "            <td>221</td>\n",
       "            <td>37</td>\n",
       "            <td>211</td>\n",
       "            <td>28</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>SVM</td>\n",
       "            <td>Site2</td>\n",
       "            <td>0.88</td>\n",
       "            <td>0.594</td>\n",
       "            <td>0.709</td>\n",
       "            <td>221</td>\n",
       "            <td>37</td>\n",
       "            <td>218</td>\n",
       "            <td>22</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>GBM</td>\n",
       "            <td>Site2</td>\n",
       "            <td>0.689</td>\n",
       "            <td>0.837</td>\n",
       "            <td>0.756</td>\n",
       "            <td>221</td>\n",
       "            <td>37</td>\n",
       "            <td>207</td>\n",
       "            <td>31</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+---------------+-----------+-----------+--------+----------+-----------+-----------+---------------+--------------+\n",
       "|     Model     | Site_Name | Precision | Recall | F1_score | No of 0's | No of 1's | Predicted 0's | Predicted1's |\n",
       "+---------------+-----------+-----------+--------+----------+-----------+-----------+---------------+--------------+\n",
       "| Decision Tree |   Site2   |   0.834   | 0.8109 |  0.821   |    221    |     37    |      215      |      30      |\n",
       "| Random Forest |   Site2   |   0.775   | 0.837  |  0.805   |    221    |     37    |      211      |      31      |\n",
       "|    Adaboost   |   Site2   |   0.736   | 0.756  |  0.746   |    221    |     37    |      211      |      28      |\n",
       "|      SVM      |   Site2   |    0.88   | 0.594  |  0.709   |    221    |     37    |      218      |      22      |\n",
       "|      GBM      |   Site2   |   0.689   | 0.837  |  0.756   |    221    |     37    |      207      |      31      |\n",
       "+---------------+-----------+-----------+--------+----------+-----------+-----------+---------------+--------------+"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8d28d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcfd226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b1c8cc5",
   "metadata": {},
   "source": [
    "### 2 >train the models on 4 randomly choosen data collated together use CV to choose appropriate hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6e2b02ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## site 2,5,7,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "79dfd99c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\spark_install\\\\lib\\\\site-packages'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "a92f7ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\Priyanka G\\\\CAPSTONE_PROJECT_WORK(2022)\\\\panneer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "373ccba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "site2 = pd.read_csv('site2.csv')\n",
    "site5 = pd.read_csv('site5.csv')\n",
    "site7 = pd.read_csv('site7.csv')\n",
    "site4 = pd.read_csv('site4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "266fdfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_com = pd.concat([site2,site5,site7,site4],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "2f092b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1932, 10)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_com.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "9bd1398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_com = data_com.loc[data_com.nwords > 2, :]\n",
    "data_com = data_com.loc[data_com.nnumbers <= 3, :]\n",
    "data_com = data_com.loc[data_com.nwords <= 10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "0065ab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_com = data_com.drop([\"Sentence\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "d1075619",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_com.drop(\"label\",axis=1)\n",
    "y = data_com.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "092b3d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=3, min_samples_leaf=5)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=DecisionTreeClassifier( max_depth=3, min_samples_leaf=5)\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cf7228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cd19b68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "site1 = pd.read_csv('site1.csv')\n",
    "site3 = pd.read_csv('site3.csv')\n",
    "site6 = pd.read_csv('site6.csv')\n",
    "site8 = pd.read_csv('site8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2ab78633",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_c = pd.concat([site1,site3,site6,site8],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c4073c",
   "metadata": {},
   "source": [
    "data_com = data_com.loc[data_com.nwords > 2, :]\n",
    "data_com = data_com.loc[data_com.nnumbers <= 3, :]\n",
    "data_com = data_com.loc[data_com.nwords <= 10, :]\n",
    "data_com = data_com.drop([\"Sentence\"],axis=1)\n",
    "X_test = data_c.drop(\"label\",axis=1)\n",
    "y_test = data_c.label\n",
    "pred2 = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6a4a5bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = data_c.drop(\"label\",axis=1)\n",
    "y_test = data_c.label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05163c12",
   "metadata": {},
   "source": [
    "## Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dcfc0f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f3168216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "77340e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator =tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ecd1ff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "    from sklearn import decomposition, datasets\n",
    "    from sklearn import tree\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2950af2d",
   "metadata": {},
   "outputs": [],
   "source": [
    " std_slc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "caea91d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "  pca = decomposition.PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "89d1b82b",
   "metadata": {},
   "outputs": [],
   "source": [
    " pipe = Pipeline(steps=[('std_slc', std_slc),\n",
    "                           ('pca', pca),\n",
    "                           ('dec_tree', estimator)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "186784ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "  n_components = list(range(1,X.shape[1]+1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9cd2646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ['gini', 'entropy']\n",
    "max_depth = [2,4,6,8,10,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "03b9413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = dict(pca__n_components=n_components,\n",
    "                      dec_tree__criterion=criterion,\n",
    "                      dec_tree__max_depth=max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7cec6d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('std_slc', StandardScaler()),\n",
       "                                       ('pca', PCA()),\n",
       "                                       ('dec_tree', DecisionTreeClassifier())]),\n",
       "             param_grid={'dec_tree__criterion': ['gini', 'entropy'],\n",
       "                         'dec_tree__max_depth': [2, 4, 6, 8, 10, 12],\n",
       "                         'pca__n_components': [1, 2, 3, 4, 5, 6, 7, 8]})"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_GS = GridSearchCV(pipe, parameters)\n",
    "clf_GS.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8a5d47b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('std_slc', StandardScaler()),\n",
       "  ('pca', PCA(n_components=8)),\n",
       "  ('dec_tree', DecisionTreeClassifier(max_depth=4))],\n",
       " 'verbose': False,\n",
       " 'std_slc': StandardScaler(),\n",
       " 'pca': PCA(n_components=8),\n",
       " 'dec_tree': DecisionTreeClassifier(max_depth=4),\n",
       " 'std_slc__copy': True,\n",
       " 'std_slc__with_mean': True,\n",
       " 'std_slc__with_std': True,\n",
       " 'pca__copy': True,\n",
       " 'pca__iterated_power': 'auto',\n",
       " 'pca__n_components': 8,\n",
       " 'pca__random_state': None,\n",
       " 'pca__svd_solver': 'auto',\n",
       " 'pca__tol': 0.0,\n",
       " 'pca__whiten': False,\n",
       " 'dec_tree__ccp_alpha': 0.0,\n",
       " 'dec_tree__class_weight': None,\n",
       " 'dec_tree__criterion': 'gini',\n",
       " 'dec_tree__max_depth': 4,\n",
       " 'dec_tree__max_features': None,\n",
       " 'dec_tree__max_leaf_nodes': None,\n",
       " 'dec_tree__min_impurity_decrease': 0.0,\n",
       " 'dec_tree__min_samples_leaf': 1,\n",
       " 'dec_tree__min_samples_split': 2,\n",
       " 'dec_tree__min_weight_fraction_leaf': 0.0,\n",
       " 'dec_tree__random_state': None,\n",
       " 'dec_tree__splitter': 'best'}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_GS.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "15f14ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=6)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_DT=DecisionTreeClassifier( max_depth=6, min_samples_leaf=1)\n",
    "model_DT.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "83f836b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    " \n",
    "# Specify the Column Names while initializing the Table\n",
    "myTable = PrettyTable([\"Model\",\"Site_Name\", \"Precision\", \"Recall\", \"F1_score\"])\n",
    "\n",
    "\n",
    "myTable.add_row([\"Decision Tree\",\"Site1\", \"1\",\"0.687\", \" 0.814\"])\n",
    "myTable.add_row([\"Decision Tree\",\"Site3\", \"0.875\",\"0.823\", \"0.848\"])\n",
    "myTable.add_row([\"Decision Tree\",\"Site6\", \"0\",\"0\", \"0\"])\n",
    "myTable.add_row([\"Decision Tree\",\"Site8\", \"1\",\"0.476\", \"0.645\"])\n",
    "myTable.add_row([\"Decision Tree\",\"combined\", \"0.954\",\"0.666\", \"0.785\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "309bb964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Model</th>\n",
       "            <th>Site_Name</th>\n",
       "            <th>Precision</th>\n",
       "            <th>Recall</th>\n",
       "            <th>F1_score</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>Decision Tree</td>\n",
       "            <td>Site1</td>\n",
       "            <td>1</td>\n",
       "            <td>0.687</td>\n",
       "            <td> 0.814</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Decision Tree</td>\n",
       "            <td>Site3</td>\n",
       "            <td>0.875</td>\n",
       "            <td>0.823</td>\n",
       "            <td>0.848</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Decision Tree</td>\n",
       "            <td>Site6</td>\n",
       "            <td>0</td>\n",
       "            <td>0</td>\n",
       "            <td>0</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Decision Tree</td>\n",
       "            <td>Site8</td>\n",
       "            <td>1</td>\n",
       "            <td>0.476</td>\n",
       "            <td>0.645</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Decision Tree</td>\n",
       "            <td>combined</td>\n",
       "            <td>0.954</td>\n",
       "            <td>0.666</td>\n",
       "            <td>0.785</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+---------------+-----------+-----------+--------+----------+\n",
       "|     Model     | Site_Name | Precision | Recall | F1_score |\n",
       "+---------------+-----------+-----------+--------+----------+\n",
       "| Decision Tree |   Site1   |     1     | 0.687  |   0.814  |\n",
       "| Decision Tree |   Site3   |   0.875   | 0.823  |  0.848   |\n",
       "| Decision Tree |   Site6   |     0     |   0    |    0     |\n",
       "| Decision Tree |   Site8   |     1     | 0.476  |  0.645   |\n",
       "| Decision Tree |  combined |   0.954   | 0.666  |  0.785   |\n",
       "+---------------+-----------+-----------+--------+----------+"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "46b86114",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST 1,3,6,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b179a110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[132   0]\n",
      " [ 10  22]]\n",
      "Prescision :  1.0\n",
      "Recall :  0.6875\n",
      "F1 score :  0.8148148148148148\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('site1.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "X_test = data2.drop(\"label\",axis=1)\n",
    "y_test = data2.label\n",
    "pred = model_DT.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "073fbeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[217   2]\n",
      " [  3  14]]\n",
      "Prescision :  0.875\n",
      "Recall :  0.8235294117647058\n",
      "F1 score :  0.8484848484848485\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('site3.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "X_test = data2.drop(\"label\",axis=1)\n",
    "y_test = data2.label\n",
    "pred = model_DT.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8f08b7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[47  0]\n",
      " [ 2  0]]\n",
      "Prescision :  0.0\n",
      "Recall :  0.0\n",
      "F1 score :  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Spark_install\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('site6.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "X_test = data2.drop(\"label\",axis=1)\n",
    "y_test = data2.label\n",
    "pred = model_DT.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "01d527ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('d:\\\\spark_install\\\\lib\\\\site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8d0096f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcee532f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a91e701d",
   "metadata": {},
   "source": [
    "### CLass imbalance issue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "761c3017",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\Priyanka G\\\\CAPSTONE_PROJECT_WORK(2022)\\\\panneer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "1c3a231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('site6.csv')\n",
    "data = data.drop([\"Sentence\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "047191f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(\"label\",axis=1)\n",
    "y = data.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "124b43ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 144, 1: 9})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "72bd7f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 144, 1: 144})\n"
     ]
    }
   ],
   "source": [
    "###site 6\n",
    "\n",
    "# instantiating the random over sampler \n",
    "ros = RandomOverSampler()\n",
    "# resampling X, y\n",
    "X_ros, y_ros = ros.fit_resample(X, y)\n",
    "# new class distribution \n",
    "print(Counter(y_ros))\n",
    "\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "fc767db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 144, 1: 144})\n"
     ]
    }
   ],
   "source": [
    "##site 8\n",
    "\n",
    "# instantiating the random over sampler \n",
    "ros = RandomOverSampler()\n",
    "# resampling X, y\n",
    "X_ros1, y_ros1 = ros.fit_resample(X, y)\n",
    "# new class distribution \n",
    "print(Counter(y_ros))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9cae36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f56cff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "345451f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[144   0]\n",
      " [144   0]]\n",
      "Prescision :  0.0\n",
      "Recall :  0.0\n",
      "F1 score :  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Spark_install\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('site6.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "\n",
    "pred = model_DT.predict(X_ros)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_ros, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_ros, pred))\n",
    "print(\"Recall : \" , recall_score(y_ros, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_ros, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "dacab799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[142   2]\n",
      " [  0 144]]\n",
      "Prescision :  0.9863013698630136\n",
      "Recall :  1.0\n",
      "F1 score :  0.993103448275862\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('site6.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "\n",
    "pred = randomForest.predict(X_ros)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_ros, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_ros, pred))\n",
    "print(\"Recall : \" , recall_score(y_ros, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_ros, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "2453dadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[143   1]\n",
      " [ 28 116]]\n",
      "Prescision :  0.9914529914529915\n",
      "Recall :  0.8055555555555556\n",
      "F1 score :  0.888888888888889\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('site6.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "\n",
    "pred = clf_AD.predict(X_ros)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_ros, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_ros, pred))\n",
    "print(\"Recall : \" , recall_score(y_ros, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_ros, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "679c35c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[144   0]\n",
      " [110  34]]\n",
      "Prescision :  1.0\n",
      "Recall :  0.2361111111111111\n",
      "F1 score :  0.38202247191011235\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('site6.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "\n",
    "pred = clf_SVM.predict(X_ros)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_ros, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_ros, pred))\n",
    "print(\"Recall : \" , recall_score(y_ros, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_ros, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "586d20a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[143   1]\n",
      " [ 34 110]]\n",
      "Prescision :  0.990990990990991\n",
      "Recall :  0.7638888888888888\n",
      "F1 score :  0.8627450980392156\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('site6.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "\n",
    "pred = clf_GBM.predict(X_ros)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_ros, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_ros, pred))\n",
    "print(\"Recall : \" , recall_score(y_ros, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_ros, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ffba42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0891aed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "ba197347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[67  0]\n",
      " [ 6  6]]\n",
      "Prescision :  1.0\n",
      "Recall :  0.5\n",
      "F1 score :  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('site8.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "X_test = data2.drop(\"label\",axis=1)\n",
    "y_test = data2.label\n",
    "pred = model_DT.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "8be2141a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[346   0]\n",
      " [161 185]]\n",
      "Prescision :  1.0\n",
      "Recall :  0.5346820809248555\n",
      "F1 score :  0.696798493408663\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('site8.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "\n",
    "pred = model_DT.predict(X_ros1)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_ros1, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_ros1, pred))\n",
    "print(\"Recall : \" , recall_score(y_ros1, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_ros1, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "c7b873ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[346   0]\n",
      " [ 86 260]]\n",
      "Prescision :  1.0\n",
      "Recall :  0.7514450867052023\n",
      "F1 score :  0.858085808580858\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('site8.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "\n",
    "pred = randomForest.predict(X_ros1)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_ros1, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_ros1, pred))\n",
    "print(\"Recall : \" , recall_score(y_ros1, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_ros1, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "9a8faa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[345   1]\n",
      " [161 185]]\n",
      "Prescision :  0.9946236559139785\n",
      "Recall :  0.5346820809248555\n",
      "F1 score :  0.6954887218045114\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('site8.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "\n",
    "pred = clf_AD.predict(X_ros1)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_ros1, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_ros1, pred))\n",
    "print(\"Recall : \" , recall_score(y_ros1, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_ros1, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "218cb36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[346   0]\n",
      " [161 185]]\n",
      "Prescision :  1.0\n",
      "Recall :  0.5346820809248555\n",
      "F1 score :  0.696798493408663\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('site8.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "\n",
    "pred = clf_SVM.predict(X_ros1)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_ros1, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_ros1, pred))\n",
    "print(\"Recall : \" , recall_score(y_ros1, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_ros1, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "21a5c379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[346   0]\n",
      " [ 86 260]]\n",
      "Prescision :  1.0\n",
      "Recall :  0.7514450867052023\n",
      "F1 score :  0.858085808580858\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('site8.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "\n",
    "pred = clf_GBM.predict(X_ros1)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_ros1, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_ros1, pred))\n",
    "print(\"Recall : \" , recall_score(y_ros1, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_ros1, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4646b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c2aa62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096a648f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01348a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234fcbcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691c0b10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bd6db5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1348cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3031472f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85d2ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae081ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2a5f8c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[463   2]\n",
      " [ 21  42]]\n",
      "Prescision :  0.9545454545454546\n",
      "Recall :  0.6666666666666666\n",
      "F1 score :  0.7850467289719627\n"
     ]
    }
   ],
   "source": [
    "data_c = pd.concat([site1,site3,site6,site8],axis=0)\n",
    "data_c = data_c.loc[data_c.nwords > 2, :]\n",
    "data_c = data_c.loc[data_c.nnumbers <= 3, :]\n",
    "data_c = data_c.loc[data_c.nwords <= 10, :]\n",
    "data_c = data_c.drop([\"Sentence\"],axis=1)\n",
    "X_test = data_c.drop(\"label\",axis=1)\n",
    "y_test = data_c.label\n",
    "pred = model_DT.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff611b1a",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b25454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "82074e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_features=0.5, n_estimators=50, oob_score=True)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomForest = ensemble.RandomForestClassifier(n_estimators=50,max_features=0.5,oob_score=True)\n",
    "randomForest.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "fa6fa788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_features=0.5, n_estimators=50, oob_score=True)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomForest.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "fd66d1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomForest.base_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "8e23d716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9741496598639455"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomForest.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "c3485cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "3bbf5d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = ensemble.RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "a07521cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "            \"n_estimators\"      : [10,20,30,40,50,60],\n",
    "            \"max_features\"      : [\"auto\", \"sqrt\", \"log2\",0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "            \"min_samples_split\" : [2,4,8],\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "689cf8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(estimator, param_grid, n_jobs=-1, cv=5)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "bf4b2f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid={'max_features': ['auto', 'sqrt', 'log2', 0.1, 0.2, 0.3,\n",
       "                                          0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
       "                         'min_samples_split': [2, 4, 8],\n",
       "                         'n_estimators': [10, 20, 30, 40, 50, 60]})"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "7d9bfc79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9755102040816327,\n",
       " {'max_features': 0.3, 'min_samples_split': 2, 'n_estimators': 60})"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_ , grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "e46dc432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_features=0.3, n_estimators=60, oob_score=True)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomForest = ensemble.RandomForestClassifier(n_estimators=60,min_samples_split = 2,max_features=0.3,oob_score=True)\n",
    "randomForest.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "977bb0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    " \n",
    "# Specify the Column Names while initializing the Table\n",
    "myTable = PrettyTable([\"Model\",\"Site_Name\", \"Precision\", \"Recall\", \"F1_score\"])\n",
    "\n",
    "\n",
    "myTable.add_row([\"RandomForest\",\"Site1\", \"0.966\",\" 0.906\", \" 0.935\"])\n",
    "myTable.add_row([\"RandomForest\",\"Site3\", \"0.736\",\"0.823\", \"0.777\"])\n",
    "myTable.add_row([\"RandomForest\",\"Site6\", \"0.986\",\" 1.0\", \" 0.993\"])\n",
    "myTable.add_row([\"RandomForest\",\"Site8\", \"1\",\" 0.765\", \"0.867\"])\n",
    "myTable.add_row([\"RandomForest\",\"combined\", \"0.9\",\"0.857\", \"0.878\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "7e601668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Model</th>\n",
       "            <th>Site_Name</th>\n",
       "            <th>Precision</th>\n",
       "            <th>Recall</th>\n",
       "            <th>F1_score</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>RandomForest</td>\n",
       "            <td>Site1</td>\n",
       "            <td>0.966</td>\n",
       "            <td> 0.906</td>\n",
       "            <td> 0.935</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>RandomForest</td>\n",
       "            <td>Site3</td>\n",
       "            <td>0.736</td>\n",
       "            <td>0.823</td>\n",
       "            <td>0.777</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>RandomForest</td>\n",
       "            <td>Site6</td>\n",
       "            <td>0.986</td>\n",
       "            <td> 1.0</td>\n",
       "            <td> 0.993</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>RandomForest</td>\n",
       "            <td>Site8</td>\n",
       "            <td>1</td>\n",
       "            <td> 0.765</td>\n",
       "            <td>0.867</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>RandomForest</td>\n",
       "            <td>combined</td>\n",
       "            <td>0.9</td>\n",
       "            <td>0.857</td>\n",
       "            <td>0.878</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+--------------+-----------+-----------+--------+----------+\n",
       "|    Model     | Site_Name | Precision | Recall | F1_score |\n",
       "+--------------+-----------+-----------+--------+----------+\n",
       "| RandomForest |   Site1   |   0.966   |  0.906 |   0.935  |\n",
       "| RandomForest |   Site3   |   0.736   | 0.823  |  0.777   |\n",
       "| RandomForest |   Site6   |   0.986   |   1.0  |   0.993  |\n",
       "| RandomForest |   Site8   |     1     |  0.765 |  0.867   |\n",
       "| RandomForest |  combined |    0.9    | 0.857  |  0.878   |\n",
       "+--------------+-----------+-----------+--------+----------+"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myTable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00280002",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "[[131   1]\n",
    " [  3  29]]\n",
    "Prescision :  0.9666666666666667\n",
    "Recall :  0.90625\n",
    "F1 score :  0.9354838709677419\n",
    "    \n",
    "[[214   5]\n",
    " [  3  14]]\n",
    "Prescision :  0.7368421052631579\n",
    "Recall :  0.8235294117647058\n",
    "F1 score :  0.7777777777777778    \n",
    "    \n",
    "    \n",
    "[[142   2]\n",
    " [  0 144]]\n",
    "Prescision :  0.9863013698630136\n",
    "Recall :  1.0\n",
    "F1 score :  0.993103448275862    \n",
    "    \n",
    "[[346   0]\n",
    " [ 81 265]]\n",
    "Prescision :  1.0\n",
    "Recall :  0.7658959537572254\n",
    "F1 score :  0.867430441898527\n",
    "    \n",
    "    \n",
    "[[459   6]\n",
    " [  9  54]]\n",
    "Prescision :  0.9\n",
    "Recall :  0.8571428571428571\n",
    "F1 score :  0.8780487804878048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "a9858163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[131   1]\n",
      " [  3  29]]\n",
      "Prescision :  0.9666666666666667\n",
      "Recall :  0.90625\n",
      "F1 score :  0.9354838709677419\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('site1.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "X_test = data2.drop(\"label\",axis=1)\n",
    "y_test = data2.label\n",
    "pred = randomForest.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "691bf414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[214   5]\n",
      " [  3  14]]\n",
      "Prescision :  0.7368421052631579\n",
      "Recall :  0.8235294117647058\n",
      "F1 score :  0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('site3.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "X_test = data2.drop(\"label\",axis=1)\n",
    "y_test = data2.label\n",
    "pred = randomForest.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "056a17ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[47  0]\n",
      " [ 0  2]]\n",
      "Prescision :  1.0\n",
      "Recall :  1.0\n",
      "F1 score :  1.0\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('site6.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "X_test = data2.drop(\"label\",axis=1)\n",
    "y_test = data2.label\n",
    "pred = randomForest.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "feb6374b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[67  0]\n",
      " [ 3  9]]\n",
      "Prescision :  1.0\n",
      "Recall :  0.75\n",
      "F1 score :  0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('site8.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "X_test = data2.drop(\"label\",axis=1)\n",
    "y_test = data2.label\n",
    "pred = randomForest.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "0c90b078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[459   6]\n",
      " [  9  54]]\n",
      "Prescision :  0.9\n",
      "Recall :  0.8571428571428571\n",
      "F1 score :  0.8780487804878048\n"
     ]
    }
   ],
   "source": [
    "data_c = pd.concat([site1,site3,site6,site8],axis=0)\n",
    "data_c = data_c.loc[data_c.nwords > 2, :]\n",
    "data_c = data_c.loc[data_c.nnumbers <= 3, :]\n",
    "data_c = data_c.loc[data_c.nwords <= 10, :]\n",
    "data_c = data_c.drop([\"Sentence\"],axis=1)\n",
    "X_test = data_c.drop(\"label\",axis=1)\n",
    "y_test = data_c.label\n",
    "pred = randomForest.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70480749",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "679febfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(n_estimators=100, random_state=0)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "d993c333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid of values to search\n",
    "grid = dict()\n",
    "grid['n_estimators'] = [10, 50, 100, 500]\n",
    "grid['learning_rate'] = [0.0001, 0.001, 0.01, 0.1, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "474e4846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "0b478979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search procedure\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "95a0a1ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9665000617055413, {'learning_rate': 0.01, 'n_estimators': 500})"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result = grid_search.fit(X, y)\n",
    "grid_result.best_score_, grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "3c72ff38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(learning_rate=0.01, n_estimators=500, random_state=0)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_AD = AdaBoostClassifier(n_estimators=500, random_state=0,learning_rate= 0.01)\n",
    "clf_AD.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "5a324d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    " \n",
    "# Specify the Column Names while initializing the Table\n",
    "myTable = PrettyTable([\"Model\",\"Site_Name\", \"Precision\", \"Recall\", \"F1_score\"])\n",
    "\n",
    "\n",
    "myTable.add_row([\"AdaBoost\",\"Site1\", \"1\",\" 0.718\", \" 0.836\"])\n",
    "myTable.add_row([\"AdaBoost\",\"Site3\", \"0.833\",\"0.882\", \"0.857\"])\n",
    "myTable.add_row([\"AdaBoost\",\"Site6\", \"0.99\",\" 0.756\", \" 0.858\"])\n",
    "myTable.add_row([\"AdaBoost\",\"Site8\", \"0.993\",\" 0.476\", \"0.644\"])\n",
    "myTable.add_row([\"AdaBoost\",\"combined\", \"0.938\",\"0.730\", \"0.821\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "6c334f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Model</th>\n",
       "            <th>Site_Name</th>\n",
       "            <th>Precision</th>\n",
       "            <th>Recall</th>\n",
       "            <th>F1_score</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>AdaBoost</td>\n",
       "            <td>Site1</td>\n",
       "            <td>1</td>\n",
       "            <td> 0.718</td>\n",
       "            <td> 0.836</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>AdaBoost</td>\n",
       "            <td>Site3</td>\n",
       "            <td>0.833</td>\n",
       "            <td>0.882</td>\n",
       "            <td>0.857</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>AdaBoost</td>\n",
       "            <td>Site6</td>\n",
       "            <td>0.99</td>\n",
       "            <td> 0.756</td>\n",
       "            <td> 0.858</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>AdaBoost</td>\n",
       "            <td>Site8</td>\n",
       "            <td>0.993</td>\n",
       "            <td> 0.476</td>\n",
       "            <td>0.644</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>AdaBoost</td>\n",
       "            <td>combined</td>\n",
       "            <td>0.938</td>\n",
       "            <td>0.730</td>\n",
       "            <td>0.821</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+----------+-----------+-----------+--------+----------+\n",
       "|  Model   | Site_Name | Precision | Recall | F1_score |\n",
       "+----------+-----------+-----------+--------+----------+\n",
       "| AdaBoost |   Site1   |     1     |  0.718 |   0.836  |\n",
       "| AdaBoost |   Site3   |   0.833   | 0.882  |  0.857   |\n",
       "| AdaBoost |   Site6   |    0.99   |  0.756 |   0.858  |\n",
       "| AdaBoost |   Site8   |   0.993   |  0.476 |  0.644   |\n",
       "| AdaBoost |  combined |   0.938   | 0.730  |  0.821   |\n",
       "+----------+-----------+-----------+--------+----------+"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dea3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[132   0]\n",
    " [  9  23]]\n",
    "Prescision :  1.0\n",
    "Recall :  0.71875\n",
    "F1 score :  0.8363636363636363\n",
    "  \n",
    "\n",
    "[[216   3]\n",
    " [  2  15]]\n",
    "Prescision :  0.8333333333333334\n",
    "Recall :  0.8823529411764706\n",
    "F1 score :  0.8571428571428571\n",
    "    \n",
    "\n",
    "    \n",
    "    [[143   1]\n",
    " [ 35 109]]\n",
    "Prescision :  0.990909090909091\n",
    "Recall :  0.7569444444444444\n",
    "F1 score :  0.858267716535433\n",
    "\n",
    "    \n",
    "[[345   1]\n",
    " [181 165]]\n",
    "Prescision :  0.9939759036144579\n",
    "Recall :  0.476878612716763\n",
    "F1 score :  0.64453125\n",
    "    \n",
    "\n",
    "[[462   3]\n",
    " [ 17  46]]\n",
    "Prescision :  0.9387755102040817\n",
    "Recall :  0.7301587301587301\n",
    "F1 score :  0.8214285714285714\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314b7f34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5399950a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4982ba71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "78c10163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[132   0]\n",
      " [  9  23]]\n",
      "Prescision :  1.0\n",
      "Recall :  0.71875\n",
      "F1 score :  0.8363636363636363\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('site1.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "X_test = data2.drop(\"label\",axis=1)\n",
    "y_test = data2.label\n",
    "pred = clf_AD.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "767d21e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[216   3]\n",
      " [  2  15]]\n",
      "Prescision :  0.8333333333333334\n",
      "Recall :  0.8823529411764706\n",
      "F1 score :  0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('site3.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "X_test = data2.drop(\"label\",axis=1)\n",
    "y_test = data2.label\n",
    "pred = clf_AD.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "414d16cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[47  0]\n",
      " [ 0  2]]\n",
      "Prescision :  1.0\n",
      "Recall :  1.0\n",
      "F1 score :  1.0\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('site6.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "X_test = data2.drop(\"label\",axis=1)\n",
    "y_test = data2.label\n",
    "pred = clf_AD.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6d81b302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[67  0]\n",
      " [10  2]]\n",
      "Prescision :  1.0\n",
      "Recall :  0.16666666666666666\n",
      "F1 score :  0.2857142857142857\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('site8.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "X_test = data2.drop(\"label\",axis=1)\n",
    "y_test = data2.label\n",
    "pred = clf_AD.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "ea617d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[462   3]\n",
      " [ 17  46]]\n",
      "Prescision :  0.9387755102040817\n",
      "Recall :  0.7301587301587301\n",
      "F1 score :  0.8214285714285714\n"
     ]
    }
   ],
   "source": [
    "data_c = pd.concat([site1,site3,site6,site8],axis=0)\n",
    "data_c = data_c.loc[data_c.nwords > 2, :]\n",
    "data_c = data_c.loc[data_c.nnumbers <= 3, :]\n",
    "data_c = data_c.loc[data_c.nwords <= 10, :]\n",
    "data_c = data_c.drop([\"Sentence\"],axis=1)\n",
    "X_test = data_c.drop(\"label\",axis=1)\n",
    "y_test = data_c.label\n",
    "pred = clf_AD.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8ef732",
   "metadata": {},
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "35241ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = svm.SVC()\n",
    "clf1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "d92bf523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[CV 1/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.891 total time=   0.1s\n",
      "[CV 2/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 3/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 4/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.884 total time=   0.0s\n",
      "[CV 5/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.884 total time=   0.0s\n",
      "[CV 1/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 2/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 3/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 4/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.884 total time=   0.0s\n",
      "[CV 5/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.884 total time=   0.0s\n",
      "[CV 1/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 2/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 3/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 4/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.884 total time=   0.0s\n",
      "[CV 5/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.884 total time=   0.0s\n",
      "[CV 1/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 2/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 3/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 4/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.884 total time=   0.0s\n",
      "[CV 5/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.884 total time=   0.0s\n",
      "[CV 1/5] END ...C=0.1, gamma=0.0001, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 2/5] END ...C=0.1, gamma=0.0001, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 3/5] END ...C=0.1, gamma=0.0001, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 4/5] END ...C=0.1, gamma=0.0001, kernel=rbf;, score=0.884 total time=   0.0s\n",
      "[CV 5/5] END ...C=0.1, gamma=0.0001, kernel=rbf;, score=0.884 total time=   0.0s\n",
      "[CV 1/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.918 total time=   0.0s\n",
      "[CV 2/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.946 total time=   0.0s\n",
      "[CV 3/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.959 total time=   0.0s\n",
      "[CV 4/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.966 total time=   0.0s\n",
      "[CV 5/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.918 total time=   0.0s\n",
      "[CV 1/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.959 total time=   0.0s\n",
      "[CV 2/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.946 total time=   0.0s\n",
      "[CV 3/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.980 total time=   0.0s\n",
      "[CV 4/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.973 total time=   0.0s\n",
      "[CV 5/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.932 total time=   0.0s\n",
      "[CV 1/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 2/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 3/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 4/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.884 total time=   0.0s\n",
      "[CV 5/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.884 total time=   0.0s\n",
      "[CV 1/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 2/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 3/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 4/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.884 total time=   0.0s\n",
      "[CV 5/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.884 total time=   0.0s\n",
      "[CV 1/5] END .....C=1, gamma=0.0001, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 2/5] END .....C=1, gamma=0.0001, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 3/5] END .....C=1, gamma=0.0001, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 4/5] END .....C=1, gamma=0.0001, kernel=rbf;, score=0.884 total time=   0.0s\n",
      "[CV 5/5] END .....C=1, gamma=0.0001, kernel=rbf;, score=0.884 total time=   0.0s\n",
      "[CV 1/5] END .........C=10, gamma=1, kernel=rbf;, score=0.925 total time=   0.0s\n",
      "[CV 2/5] END .........C=10, gamma=1, kernel=rbf;, score=0.932 total time=   0.0s\n",
      "[CV 3/5] END .........C=10, gamma=1, kernel=rbf;, score=0.946 total time=   0.0s\n",
      "[CV 4/5] END .........C=10, gamma=1, kernel=rbf;, score=0.973 total time=   0.0s\n",
      "[CV 5/5] END .........C=10, gamma=1, kernel=rbf;, score=0.939 total time=   0.0s\n",
      "[CV 1/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.973 total time=   0.0s\n",
      "[CV 2/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.959 total time=   0.0s\n",
      "[CV 3/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.973 total time=   0.0s\n",
      "[CV 4/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.973 total time=   0.0s\n",
      "[CV 5/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.946 total time=   0.0s\n",
      "[CV 1/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.973 total time=   0.0s\n",
      "[CV 2/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.952 total time=   0.0s\n",
      "[CV 3/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.973 total time=   0.0s\n",
      "[CV 4/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.966 total time=   0.0s\n",
      "[CV 5/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.946 total time=   0.0s\n",
      "[CV 1/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 2/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 3/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 4/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.884 total time=   0.0s\n",
      "[CV 5/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.884 total time=   0.0s\n",
      "[CV 1/5] END ....C=10, gamma=0.0001, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 2/5] END ....C=10, gamma=0.0001, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 3/5] END ....C=10, gamma=0.0001, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 4/5] END ....C=10, gamma=0.0001, kernel=rbf;, score=0.884 total time=   0.0s\n",
      "[CV 5/5] END ....C=10, gamma=0.0001, kernel=rbf;, score=0.884 total time=   0.0s\n",
      "[CV 1/5] END ........C=100, gamma=1, kernel=rbf;, score=0.925 total time=   0.0s\n",
      "[CV 2/5] END ........C=100, gamma=1, kernel=rbf;, score=0.932 total time=   0.0s\n",
      "[CV 3/5] END ........C=100, gamma=1, kernel=rbf;, score=0.946 total time=   0.0s\n",
      "[CV 4/5] END ........C=100, gamma=1, kernel=rbf;, score=0.973 total time=   0.0s\n",
      "[CV 5/5] END ........C=100, gamma=1, kernel=rbf;, score=0.939 total time=   0.0s\n",
      "[CV 1/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.952 total time=   0.0s\n",
      "[CV 2/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.932 total time=   0.0s\n",
      "[CV 3/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.952 total time=   0.0s\n",
      "[CV 4/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.959 total time=   0.0s\n",
      "[CV 5/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.925 total time=   0.0s\n",
      "[CV 1/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.980 total time=   0.0s\n",
      "[CV 2/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.959 total time=   0.0s\n",
      "[CV 3/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.980 total time=   0.0s\n",
      "[CV 4/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.973 total time=   0.0s\n",
      "[CV 5/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.946 total time=   0.0s\n",
      "[CV 1/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.966 total time=   0.0s\n",
      "[CV 2/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.952 total time=   0.0s\n",
      "[CV 3/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.973 total time=   0.0s\n",
      "[CV 4/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.966 total time=   0.0s\n",
      "[CV 5/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.939 total time=   0.0s\n",
      "[CV 1/5] END ...C=100, gamma=0.0001, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 2/5] END ...C=100, gamma=0.0001, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 3/5] END ...C=100, gamma=0.0001, kernel=rbf;, score=0.891 total time=   0.0s\n",
      "[CV 4/5] END ...C=100, gamma=0.0001, kernel=rbf;, score=0.884 total time=   0.0s\n",
      "[CV 5/5] END ...C=100, gamma=0.0001, kernel=rbf;, score=0.884 total time=   0.0s\n",
      "[CV 1/5] END .......C=1000, gamma=1, kernel=rbf;, score=0.925 total time=   0.0s\n",
      "[CV 2/5] END .......C=1000, gamma=1, kernel=rbf;, score=0.932 total time=   0.0s\n",
      "[CV 3/5] END .......C=1000, gamma=1, kernel=rbf;, score=0.946 total time=   0.0s\n",
      "[CV 4/5] END .......C=1000, gamma=1, kernel=rbf;, score=0.973 total time=   0.0s\n",
      "[CV 5/5] END .......C=1000, gamma=1, kernel=rbf;, score=0.939 total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END .....C=1000, gamma=0.1, kernel=rbf;, score=0.925 total time=   0.0s\n",
      "[CV 2/5] END .....C=1000, gamma=0.1, kernel=rbf;, score=0.939 total time=   0.0s\n",
      "[CV 3/5] END .....C=1000, gamma=0.1, kernel=rbf;, score=0.932 total time=   0.0s\n",
      "[CV 4/5] END .....C=1000, gamma=0.1, kernel=rbf;, score=0.959 total time=   0.0s\n",
      "[CV 5/5] END .....C=1000, gamma=0.1, kernel=rbf;, score=0.959 total time=   0.0s\n",
      "[CV 1/5] END ....C=1000, gamma=0.01, kernel=rbf;, score=0.973 total time=   0.0s\n",
      "[CV 2/5] END ....C=1000, gamma=0.01, kernel=rbf;, score=0.966 total time=   0.0s\n",
      "[CV 3/5] END ....C=1000, gamma=0.01, kernel=rbf;, score=0.973 total time=   0.0s\n",
      "[CV 4/5] END ....C=1000, gamma=0.01, kernel=rbf;, score=0.973 total time=   0.0s\n",
      "[CV 5/5] END ....C=1000, gamma=0.01, kernel=rbf;, score=0.952 total time=   0.0s\n",
      "[CV 1/5] END ...C=1000, gamma=0.001, kernel=rbf;, score=0.973 total time=   0.0s\n",
      "[CV 2/5] END ...C=1000, gamma=0.001, kernel=rbf;, score=0.952 total time=   0.0s\n",
      "[CV 3/5] END ...C=1000, gamma=0.001, kernel=rbf;, score=0.980 total time=   0.0s\n",
      "[CV 4/5] END ...C=1000, gamma=0.001, kernel=rbf;, score=0.966 total time=   0.0s\n",
      "[CV 5/5] END ...C=1000, gamma=0.001, kernel=rbf;, score=0.939 total time=   0.0s\n",
      "[CV 1/5] END ..C=1000, gamma=0.0001, kernel=rbf;, score=0.966 total time=   0.0s\n",
      "[CV 2/5] END ..C=1000, gamma=0.0001, kernel=rbf;, score=0.952 total time=   0.0s\n",
      "[CV 3/5] END ..C=1000, gamma=0.0001, kernel=rbf;, score=0.973 total time=   0.0s\n",
      "[CV 4/5] END ..C=1000, gamma=0.0001, kernel=rbf;, score=0.966 total time=   0.0s\n",
      "[CV 5/5] END ..C=1000, gamma=0.0001, kernel=rbf;, score=0.939 total time=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': [0.1, 1, 10, 100, 1000],\n",
       "                         'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
       "                         'kernel': ['rbf']},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining parameter range\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000],\n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'kernel': ['rbf']}\n",
    " \n",
    "grid = GridSearchCV(clf1, param_grid, refit = True, verbose = 3)\n",
    " \n",
    "# fitting the model for grid search\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "8d9c1dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}, SVC(C=100, gamma=0.01))"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_,grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "57b57186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, gamma=0.01)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_SVM = svm.SVC(C=100,gamma=0.01)\n",
    "clf_SVM.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c886790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    " \n",
    "# Specify the Column Names while initializing the Table\n",
    "myTable = PrettyTable([\"Model\",\"Site_Name\", \"Precision\", \"Recall\", \"F1_score\"])\n",
    "\n",
    "\n",
    "myTable.add_row([\"SVM\",\"Site1\", \"1\",\" 0.718\", \" 0.836\"])\n",
    "myTable.add_row([\"SVM\",\"Site3\", \"0.833\",\"0.882\", \"0.857\"])\n",
    "myTable.add_row([\"SVM\",\"Site6\", \"0.99\",\" 0.756\", \" 0.858\"])\n",
    "myTable.add_row([\"SVM\",\"Site8\", \"0.993\",\" 0.476\", \"0.644\"])\n",
    "myTable.add_row([\"SVM\",\"combined\", \"0.938\",\"0.730\", \"0.821\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9877e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[132   0]\n",
    " [  9  23]]\n",
    "Prescision :  1.0\n",
    "Recall :  0.71875\n",
    "F1 score :  0.8363636363636363\n",
    "    \n",
    "[[213   6]\n",
    " [  3  14]]\n",
    "Prescision :  0.7\n",
    "Recall :  0.8235294117647058\n",
    "F1 score :  0.7567567567567567\n",
    "   \n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "[[346   0]\n",
    " [181 165]]\n",
    "Prescision :  1.0\n",
    "Recall :  0.476878612716763\n",
    "F1 score :  0.6457925636007827\n",
    "\n",
    "    \n",
    "[[459   6]\n",
    " [ 18  45]]\n",
    "Prescision :  0.8823529411764706\n",
    "Recall :  0.7142857142857143\n",
    "F1 score :  0.7894736842105262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "18d485bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[132   0]\n",
      " [  9  23]]\n",
      "Prescision :  1.0\n",
      "Recall :  0.71875\n",
      "F1 score :  0.8363636363636363\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('site1.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "X_test = data2.drop(\"label\",axis=1)\n",
    "y_test = data2.label\n",
    "pred = clf_SVM.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "1897bed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[213   6]\n",
      " [  3  14]]\n",
      "Prescision :  0.7\n",
      "Recall :  0.8235294117647058\n",
      "F1 score :  0.7567567567567567\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('site3.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "X_test = data2.drop(\"label\",axis=1)\n",
    "y_test = data2.label\n",
    "pred = clf_SVM.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "13628757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[47  0]\n",
      " [ 0  2]]\n",
      "Prescision :  1.0\n",
      "Recall :  1.0\n",
      "F1 score :  1.0\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('site6.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "X_test = data2.drop(\"label\",axis=1)\n",
    "y_test = data2.label\n",
    "pred = clf_SVM.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "35c34d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[67  0]\n",
      " [ 6  6]]\n",
      "Prescision :  1.0\n",
      "Recall :  0.5\n",
      "F1 score :  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('site8.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "X_test = data2.drop(\"label\",axis=1)\n",
    "y_test = data2.label\n",
    "pred = clf_SVM.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "89336278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[459   6]\n",
      " [ 18  45]]\n",
      "Prescision :  0.8823529411764706\n",
      "Recall :  0.7142857142857143\n",
      "F1 score :  0.7894736842105262\n"
     ]
    }
   ],
   "source": [
    "data_c = pd.concat([site1,site3,site6,site8],axis=0)\n",
    "data_c = data_c.loc[data_c.nwords > 2, :]\n",
    "data_c = data_c.loc[data_c.nnumbers <= 3, :]\n",
    "data_c = data_c.loc[data_c.nwords <= 10, :]\n",
    "data_c = data_c.drop([\"Sentence\"],axis=1)\n",
    "X_test = data_c.drop(\"label\",axis=1)\n",
    "y_test = data_c.label\n",
    "pred = clf_SVM.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c34940c",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "db84facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "c97ce248",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test = {'max_depth':[2,3,4,5,6,7] ,\n",
    "           'learning_rate':[0.15,0.1,0.05,0.01,0.005,0.001], 'n_estimators':[100,250,500,750,1000,1250,1500,1750],\n",
    "          }\n",
    "tuning = GridSearchCV(estimator =clf2, param_grid = p_test, scoring='accuracy',n_jobs=4, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "fb43de38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=GradientBoostingClassifier(learning_rate=1.0,\n",
       "                                                  max_depth=1, random_state=0),\n",
       "             n_jobs=4,\n",
       "             param_grid={'learning_rate': [0.15, 0.1, 0.05, 0.01, 0.005, 0.001],\n",
       "                         'max_depth': [2, 3, 4, 5, 6, 7],\n",
       "                         'n_estimators': [100, 250, 500, 750, 1000, 1250, 1500,\n",
       "                                          1750]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuning.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "aff398fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1750},\n",
       " 0.9768707482993196)"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuning.best_params_, tuning.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "d9ca2312",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_GBM = GradientBoostingClassifier(n_estimators=1750, learning_rate=0.01,max_depth=3, random_state=0).fit(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "f5830b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[131   1]\n",
      " [  3  29]]\n",
      "Prescision :  0.9666666666666667\n",
      "Recall :  0.90625\n",
      "F1 score :  0.9354838709677419\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('site1.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "X_test = data2.drop(\"label\",axis=1)\n",
    "y_test = data2.label\n",
    "pred = clf_GBM.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "e2a8188f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[217   2]\n",
      " [  4  13]]\n",
      "Prescision :  0.8666666666666667\n",
      "Recall :  0.7647058823529411\n",
      "F1 score :  0.8125\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('site3.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "X_test = data2.drop(\"label\",axis=1)\n",
    "y_test = data2.label\n",
    "pred = clf_GBM.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "51d9d071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[47  0]\n",
      " [ 2  0]]\n",
      "Prescision :  0.0\n",
      "Recall :  0.0\n",
      "F1 score :  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Spark_install\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('site6.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "X_test = data2.drop(\"label\",axis=1)\n",
    "y_test = data2.label\n",
    "pred = clf_GBM.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "5e8c3280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[67  0]\n",
      " [ 3  9]]\n",
      "Prescision :  1.0\n",
      "Recall :  0.75\n",
      "F1 score :  0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('site8.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "X_test = data2.drop(\"label\",axis=1)\n",
    "y_test = data2.label\n",
    "pred = clf_GBM.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "922ba503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[462   3]\n",
      " [ 12  51]]\n",
      "Prescision :  0.9444444444444444\n",
      "Recall :  0.8095238095238095\n",
      "F1 score :  0.8717948717948718\n"
     ]
    }
   ],
   "source": [
    "data_c = pd.concat([site1,site3,site6,site8],axis=0)\n",
    "data_c = data_c.loc[data_c.nwords > 2, :]\n",
    "data_c = data_c.loc[data_c.nnumbers <= 3, :]\n",
    "data_c = data_c.loc[data_c.nwords <= 10, :]\n",
    "data_c = data_c.drop([\"Sentence\"],axis=1)\n",
    "X_test = data_c.drop(\"label\",axis=1)\n",
    "y_test = data_c.label\n",
    "pred =clf_GBM.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55ee933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28a4197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d6a3277",
   "metadata": {},
   "source": [
    "## 3>Test the model on a dataset on gobi and chicken manchurian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b83d9b",
   "metadata": {},
   "source": [
    "https://www.indianhealthyrecipes.com/chicken-manchurian-recipe/\n",
    "https://hebbarskitchen.com/dry-gobi-manchurian-recipe/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "6be21f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Priyanka G\\\\CAPSTONE_PROJECT_WORK(2022)\\\\panneer'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d38cb1",
   "metadata": {},
   "source": [
    "## gobi machurian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "16ec1c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('gobi.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "X_test = data2.drop(\"label\",axis=1)\n",
    "y_test = data2.label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "02f69e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[104   0]\n",
      " [  5  15]]\n",
      "Prescision :  1.0\n",
      "Recall :  0.75\n",
      "F1 score :  0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "pred = model_DT.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "5a38b1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[104   0]\n",
      " [  3  17]]\n",
      "Prescision :  1.0\n",
      "Recall :  0.85\n",
      "F1 score :  0.9189189189189189\n"
     ]
    }
   ],
   "source": [
    "pred = randomForest.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "d5eda77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[104   0]\n",
      " [  5  15]]\n",
      "Prescision :  1.0\n",
      "Recall :  0.75\n",
      "F1 score :  0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "pred = clf_AD.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "f1e3db93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[104   0]\n",
      " [  5  15]]\n",
      "Prescision :  1.0\n",
      "Recall :  0.75\n",
      "F1 score :  0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "pred = clf_SVM.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "b673d389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[104   0]\n",
      " [  3  17]]\n",
      "Prescision :  1.0\n",
      "Recall :  0.85\n",
      "F1 score :  0.9189189189189189\n"
     ]
    }
   ],
   "source": [
    "pred = clf_GBM.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfc626e",
   "metadata": {},
   "source": [
    "### chicken machurian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "046f2ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('chicken.csv')\n",
    "data2 = data2.loc[data2.nwords > 2, :]\n",
    "data2 = data2.loc[data2.nnumbers <= 3, :]\n",
    "data2 = data2.loc[data2.nwords <= 10, :]\n",
    "data2 = data2.drop([\"Sentence\"],axis=1)\n",
    "X_test = data2.drop(\"label\",axis=1)\n",
    "y_test = data2.label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "91470c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[227   1]\n",
      " [  4  16]]\n",
      "Prescision :  0.9411764705882353\n",
      "Recall :  0.8\n",
      "F1 score :  0.8648648648648648\n"
     ]
    }
   ],
   "source": [
    "pred = model_DT.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "323caf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[226   2]\n",
      " [  2  18]]\n",
      "Prescision :  0.9\n",
      "Recall :  0.9\n",
      "F1 score :  0.9\n"
     ]
    }
   ],
   "source": [
    "pred = randomForest.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "52e05b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[226   2]\n",
      " [  2  18]]\n",
      "Prescision :  0.9\n",
      "Recall :  0.9\n",
      "F1 score :  0.9\n"
     ]
    }
   ],
   "source": [
    "pred = clf_AD.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "942d3b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[226   2]\n",
      " [  2  18]]\n",
      "Prescision :  0.9\n",
      "Recall :  0.9\n",
      "F1 score :  0.9\n"
     ]
    }
   ],
   "source": [
    "pred = clf_SVM.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "5b923b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[227   1]\n",
      " [  3  17]]\n",
      "Prescision :  0.9444444444444444\n",
      "Recall :  0.85\n",
      "F1 score :  0.8947368421052632\n"
     ]
    }
   ],
   "source": [
    "pred = clf_GBM.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred))\n",
    "\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#precision_score(y_test, pred)\n",
    "print(\"Prescision : \" , precision_score(y_test, pred))\n",
    "print(\"Recall : \" , recall_score(y_test, pred))\n",
    "\n",
    "# To compute the F1 score, simply call the f1_score() function:\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score : \" ,f1_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccddf0e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd624b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc7dc27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
